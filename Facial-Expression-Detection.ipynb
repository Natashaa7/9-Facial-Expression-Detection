{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e39ad58d-2f4d-4478-aa50-b686ae3e4ed8",
   "metadata": {},
   "source": [
    "# 9 Facial Expressions You Need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf1f707-1dda-4f97-a36b-3258997e1c7f",
   "metadata": {},
   "source": [
    "# ❇️ Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2980079e-ffb7-4be9-aea6-3dac552f581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8198636e-2e8a-4a2f-95c5-8867dba21144",
   "metadata": {},
   "source": [
    "- **os** → Handles file and folder operations (e.g., list, create, delete).  \n",
    "- **cv2** → OpenCV library used for image processing (read, display, edit images).  \n",
    "- **numpy** → Used for numerical operations and arrays (images are stored as NumPy arrays)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d084db9-7e24-48b8-a505-fd154629118b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, target_size=(640, 640), to_gray=False):\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        return None\n",
    "    if to_gray:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, target_size) # resize\n",
    "\n",
    "    \n",
    "    # Normalize for training (not saving)\n",
    "    img_normalized = img / 255.0  # Scale to 0-1 for uint8 images\n",
    "\n",
    "    # Convert back to uint8 for saving\n",
    "    img_save = (img_normalized * 255).astype(\"uint8\")\n",
    "\n",
    "    # Save properly\n",
    "    cv2.imwrite(os.path.join(output_img_path, file), cv2.cvtColor(img_save, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244bcf78-3e54-4e3e-b6ca-846c234ee0e1",
   "metadata": {},
   "source": [
    "### 🧩 Function: `preprocess_image`\n",
    "\n",
    "**Purpose:**  \n",
    "Preprocesses an image before training — resizing, color conversion, and normalization.\n",
    "\n",
    "**Steps:**\n",
    "1. **Read image**\n",
    "   - `cv2.imread(image_path)` loads the image.\n",
    "   - Returns `None` if the file doesn't exist.\n",
    "\n",
    "2. **Color conversion**\n",
    "   - If `to_gray=True`: convert to grayscale → `cv2.COLOR_BGR2GRAY`\n",
    "   - Else: convert from BGR → RGB (OpenCV loads in BGR by default).\n",
    "\n",
    "3. **Resize**\n",
    "   - `cv2.resize(img, target_size)` resizes the image (default 640×640).\n",
    "\n",
    "4. **Normalize**\n",
    "   - `img / 255.0` scales pixel values from `0–255` to `0–1` for training.\n",
    "\n",
    "5. **Convert back to uint8**\n",
    "   - `(img_normalized * 255).astype(\"uint8\")` brings values back to 8-bit format for saving.\n",
    "\n",
    "6. **Save processed image**\n",
    "   - `cv2.imwrite()` saves the image after converting RGB → BGR again for OpenCV.\n",
    "\n",
    "**Note:**  \n",
    "- `output_img_path` and `file` should be defined before calling `cv2.imwrite()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48b41456-4c67-4966-85a8-ecf2f18a70c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train images...\n",
      "Processing test images...\n",
      "Processing valid images...\n",
      "✅ All images preprocessed and labels copied successfully!\n"
     ]
    }
   ],
   "source": [
    "# folder\n",
    "folders = [\"train\", \"test\", \"valid\"]\n",
    "base_path = \"/Users/natashababu/Documents/internship/facial-expression/9-Facial-Expressions-You-Need\"\n",
    "save_path = \"/Users/natashababu/Documents/internship/facial-expression/preprocessed-dataset\"\n",
    "\n",
    "for folder in folders:\n",
    "    input_img_path = os.path.join(base_path, folder, \"images\")\n",
    "    input_lbl_path = os.path.join(base_path, folder, \"labels\")\n",
    "\n",
    "    output_img_path = os.path.join(save_path, folder, \"images\")\n",
    "    output_lbl_path = os.path.join(save_path, folder, \"labels\")\n",
    "\n",
    "    os.makedirs(output_img_path, exist_ok=True)\n",
    "    os.makedirs(output_lbl_path, exist_ok=True)\n",
    "\n",
    "    print(f\"Processing {folder} images...\")\n",
    "    for file in os.listdir(input_img_path):\n",
    "        if not file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            continue\n",
    "            \n",
    "        img_path = os.path.join(input_img_path, file)\n",
    "        processed = preprocess_image(img_path)\n",
    "\n",
    "        # Copy corresponding label file\n",
    "        label_name = os.path.splitext(file)[0] + \".txt\"\n",
    "        src_label = os.path.join(input_lbl_path, label_name)\n",
    "        dst_label = os.path.join(output_lbl_path, label_name)\n",
    "\n",
    "        if os.path.exists(src_label):\n",
    "            with open(src_label, \"r\") as f:\n",
    "                label_data = f.read()\n",
    "            with open(dst_label, \"w\") as f:\n",
    "                f.write(label_data)\n",
    "\n",
    "print(\"✅ All images preprocessed and labels copied successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cff2be0-85a6-4a32-8221-88e525a9df69",
   "metadata": {},
   "source": [
    "### 🧩 Code Summary\n",
    "\n",
    "**Purpose:**  \n",
    "This code preprocesses all images and copies their corresponding label files for the `train`, `test`, and `valid` datasets, creating a ready-to-use dataset for training machine learning models.\n",
    "\n",
    "**Steps:**\n",
    "1. **Define paths**  \n",
    "   - `base_path` points to the original dataset.  \n",
    "   - `save_path` is where the preprocessed dataset will be stored.  \n",
    "   - The code works with separate folders for `train`, `test`, and `valid`.\n",
    "\n",
    "2. **Loop through folders**  \n",
    "   - For each folder, input and output paths for images and labels are set.  \n",
    "   - Output directories are created if they don’t exist.\n",
    "\n",
    "3. **Process images**  \n",
    "   - Iterate through all files in the input image folder.  \n",
    "   - Skip files that are not images (`.jpg`, `.jpeg`, `.png`).  \n",
    "   - Each image is preprocessed using the `preprocess_image()` function (resizing, color conversion, and normalization).\n",
    "\n",
    "4. **Copy labels**  \n",
    "   - For each image, find the corresponding `.txt` label file.  \n",
    "   - Read the label content and write it to the output label folder, preserving the original file structure.\n",
    "\n",
    "5. **Completion message**  \n",
    "   - After all images and labels are processed, a success message is printed.\n",
    "\n",
    "**Result:**  \n",
    "A clean, preprocessed dataset with images resized and normalized, and all labels copied correctly, ready for training models while keeping the original folder structure intact.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "889a9c57-9226-4b5c-b080-33460c5f24e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (2.8.0)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (0.23.0)\n",
      "Requirement already satisfied: torchaudio in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "# For PyTorch >=2.0 (with MPS support)\n",
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e595f4-cb8d-4b28-993c-e0ca2339dbea",
   "metadata": {},
   "source": [
    "### 🧩 Code Explanation\n",
    "\n",
    "**Purpose:**  \n",
    "Installs PyTorch and related libraries for deep learning, specifically compatible with **PyTorch ≥2.0** and **MPS (Metal Performance Shaders) support** on macOS.\n",
    "\n",
    "**Libraries:**\n",
    "\n",
    "- torch → Core PyTorch library for tensor operations and building neural networks.\n",
    "- torchvision → Utilities for computer vision tasks (image transformations, pretrained models, datasets).\n",
    "- torchaudio → Utilities for audio processing with PyTorch (datasets, transforms, models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9aa6b721-2808-4e9f-be46-b16462a2bd13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.backends.mps.is_available()  # Should return True\n",
    "torch.backends.mps.is_built()      # Should return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d0cae5-fc4f-4ba3-8fc6-c163e8e4dbfb",
   "metadata": {},
   "source": [
    "### 🧩 Code Explanation\n",
    "\n",
    "**Purpose:**  \n",
    "Checks if **MPS (Metal Performance Shaders)** is available and built on your Mac for PyTorch GPU acceleration.\n",
    "\n",
    "**Code:**\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# Check if MPS (Apple GPU support) is available\n",
    "torch.backends.mps.is_available()  # Returns True if MPS can be used\n",
    "\n",
    "# Check if MPS is built in your PyTorch installation\n",
    "torch.backends.mps.is_built()      # Returns True if PyTorch was compiled with MPS support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97df88e8-586b-4005-942f-3cc7bb7968b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.205 🚀 Python-3.10.18 torch-2.8.0 MPS (Apple M3 Pro)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=True, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=data.yaml, degrees=10, deterministic=True, device=mps, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=320, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.1, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train3, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/Users/natashababu/Documents/internship/facial-expression/runs/detect/train3, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=2.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=9\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    753067  ultralytics.nn.modules.head.Detect           [9, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,012,603 parameters, 3,012,587 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 264.2±194.3 MB/s, size: 96.0 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/natashababu/Documents/internship/facial-expression/preprocessed-dataset/train/labels.cache... 64864 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 64864/64864 113.4Mit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 248.8±136.7 MB/s, size: 93.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/natashababu/Documents/internship/facial-expression/preprocessed-dataset/valid/labels.cache... 1720 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 1720/1720 4.3Mit/s 0.0s0s\n",
      "Plotting labels to /Users/natashababu/Documents/internship/facial-expression/runs/detect/train3/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 320 train, 320 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m/Users/natashababu/Documents/internship/facial-expression/runs/detect/train3\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/10       1.2G     0.9929       2.28      1.319         16        320: 100% ━━━━━━━━━━━━ 4054/4054 2.1it/s 32:53<0.4ss\n",
      "WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 2% ──────────── 1/54 0.0it/s 20.2s<59:20WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 4% ──────────── 2/54 0.1it/s 27.3s<16:35WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 6% ╸─────────── 3/54 0.1it/s 34.5s<10:52WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 7% ╸─────────── 4/54 0.1it/s 41.1s<8:19WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 9% ━─────────── 5/54 0.1it/s 47.5s<6:58WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 11% ━─────────── 6/54 0.1it/s 53.8s<6:10WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 13% ━╸────────── 7/54 0.1it/s 59.3s<5:23WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 15% ━╸────────── 8/54 0.1it/s 1:07<5:25WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 17% ━━────────── 9/54 0.1it/s 1:14<5:21WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 19% ━━────────── 10/54 0.1it/s 1:21<5:14WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 20% ━━────────── 11/54 0.1it/s 1:27<4:53WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 22% ━━╸───────── 12/54 0.2it/s 1:34<4:38WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 24% ━━╸───────── 13/54 0.2it/s 1:40<4:30WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 26% ━━━───────── 14/54 0.2it/s 1:47<4:25WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 28% ━━━───────── 15/54 0.1it/s 1:54<4:26WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 30% ━━━╸──────── 16/54 0.1it/s 2:01<4:14WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 31% ━━━╸──────── 17/54 0.2it/s 2:06<3:57WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 33% ━━━━──────── 18/54 0.2it/s 2:12<3:47WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 35% ━━━━──────── 19/54 0.2it/s 2:19<3:44WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 37% ━━━━──────── 20/54 0.2it/s 2:25<3:33WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 39% ━━━━╸─────── 21/54 0.2it/s 2:31<3:23WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 41% ━━━━╸─────── 22/54 0.2it/s 2:37<3:16WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 43% ━━━━━─────── 23/54 0.2it/s 2:42<3:00WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 44% ━━━━━─────── 24/54 0.2it/s 2:48<2:50WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 46% ━━━━━╸────── 25/54 0.2it/s 2:54<2:53WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 48% ━━━━━╸────── 26/54 0.2it/s 2:59<2:36WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 50% ━━━━━━────── 27/54 0.2it/s 3:05<2:35WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 52% ━━━━━━────── 28/54 0.2it/s 3:11<2:27WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 54% ━━━━━━────── 29/54 0.2it/s 3:17<2:26WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 56% ━━━━━━╸───── 30/54 0.2it/s 3:23<2:22WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 57% ━━━━━━╸───── 31/54 0.2it/s 3:29<2:16WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 59% ━━━━━━━───── 32/54 0.2it/s 3:35<2:08WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 61% ━━━━━━━───── 33/54 0.2it/s 3:40<1:58WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 63% ━━━━━━━╸──── 34/54 0.2it/s 3:46<1:57WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 65% ━━━━━━━╸──── 35/54 0.2it/s 3:51<1:44WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 67% ━━━━━━━━──── 36/54 0.2it/s 3:57<1:42WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 69% ━━━━━━━━──── 37/54 0.2it/s 4:02<1:34WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 70% ━━━━━━━━──── 38/54 0.2it/s 4:08<1:26WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 72% ━━━━━━━━╸─── 39/54 0.2it/s 4:14<1:24WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 74% ━━━━━━━━╸─── 40/54 0.2it/s 4:20<1:20WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 76% ━━━━━━━━━─── 41/54 0.2it/s 4:28<1:21WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 78% ━━━━━━━━━─── 42/54 0.2it/s 4:33<1:11WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 80% ━━━━━━━━━╸── 43/54 0.2it/s 4:41<1:12WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 81% ━━━━━━━━━╸── 44/54 0.1it/s 5:18<1:26WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 83% ━━━━━━━━━━── 45/54 0.1it/s 5:23<1:06WARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 85% ━━━━━━━━━━── 46/54 0.1it/s 5:29<54.1sWARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 87% ━━━━━━━━━━── 47/54 0.2it/s 5:35<46.2sWARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 89% ━━━━━━━━━━╸─ 48/54 0.2it/s 5:41<38.3sWARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 91% ━━━━━━━━━━╸─ 49/54 0.2it/s 5:46<30.5sWARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 93% ━━━━━━━━━━━─ 50/54 0.2it/s 5:52<24.0sWARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 94% ━━━━━━━━━━━─ 51/54 0.2it/s 5:59<18.8sWARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 96% ━━━━━━━━━━━╸ 52/54 0.1it/s 6:07<13.4sWARNING ⚠️ NMS time limit 3.600s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 98% ━━━━━━━━━━━╸ 53/54 0.2it/s 6:13<6.4sWARNING ⚠️ NMS time limit 3.200s exceeded\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 54/54 0.1it/s 6:36\n",
      "                   all       1720       1720      0.387      0.185      0.155        0.1\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/10      1.22G     0.9398      1.711      1.239         16        320: 22% ━━╸───────── 889/4054 2.1it/s 21:39<25:291"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "## 1️⃣ Load pretrained YOLOv8 nano model\n",
    "# 'n' = nano → fastest and lightest for small datasets\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# 2️⃣ Training parameters optimized\n",
    "model.train(\n",
    "    data=\"data.yaml\",     # Path to your dataset YAML\n",
    "    epochs=10,           # Max epochs; early stopping will stop sooner\n",
    "    imgsz=320,            # Image size (resize all images)\n",
    "    device=\"mps\",         # Use Apple GPU (MPS)\n",
    "    patience=10,          # Early stopping if val mAP doesn't improve for 10 epochs\n",
    "    batch=16,             # Adjust batch size if you get memory errors\n",
    "    augment=True,         # Enable YOLO's built-in augmentation\n",
    "    hsv_h=0.015,          # Hue augmentation\n",
    "    hsv_s=0.7,            # Saturation\n",
    "    hsv_v=0.4,            # Brightness/value\n",
    "    degrees=10,           # Random rotation\n",
    "    translate=0.1,        # Random translation\n",
    "    scale=0.5,            # Random scaling\n",
    "    shear=2.0,            # Random shear\n",
    "    fliplr=0.5,           # Horizontal flip\n",
    "    flipud=0.0,           # Vertical flip\n",
    "    mosaic=1.0,           # Mosaic augmentation\n",
    "    mixup=0.1             # Mixup augmentation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb0c9e4-93bf-4554-8fec-770832e608f9",
   "metadata": {},
   "source": [
    "### 🧩 YOLOv8 Training Code Explanation\n",
    "\n",
    "**Purpose:**  \n",
    "Load a pretrained YOLOv8 nano model and train it on a custom dataset with data augmentation and optimized parameters.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Import YOLO: Loads the YOLO class from the Ultralytics library.\n",
    "#### 2. Load pretrained model\n",
    "- \"yolov8n.pt\" → nano model (smallest and fastest, ideal for small datasets).\n",
    "- Pretrained weights help the model converge faster.\n",
    "#### 3. Train the model\n",
    "| Parameter           | Description                                              |\n",
    "| ------------------- | -------------------------------------------------------- |\n",
    "| `data`              | Path to dataset YAML (images, labels, class names)       |\n",
    "| `epochs`            | Maximum training cycles                                  |\n",
    "| `imgsz`             | Resize all images to this size                           |\n",
    "| `device`            | `\"mps\"` → Use Apple GPU for faster training              |\n",
    "| `patience`          | Early stopping if validation performance doesn’t improve |\n",
    "| `batch`             | Number of images per batch                               |\n",
    "| `augment`           | Enable YOLO’s built-in data augmentation                 |\n",
    "| `hsv_h/s/v`         | Adjust hue, saturation, brightness randomly              |\n",
    "| `degrees`           | Random rotation                                          |\n",
    "| `translate`         | Random image shift                                       |\n",
    "| `scale`             | Random scaling                                           |\n",
    "| `shear`             | Random shear transformation                              |\n",
    "| `fliplr` / `flipud` | Horizontal/vertical flips                                |\n",
    "| `mosaic`            | Combine 4 images into 1 for better generalization        |\n",
    "| `mixup`             | Blend two images and labels to reduce overfitting        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6d1357-b2ce-4c59-8689-c95f7d82aedb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_env]",
   "language": "python",
   "name": "conda-env-tf_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
